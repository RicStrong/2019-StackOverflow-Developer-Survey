# -*- coding: utf-8 -*-
"""EAfinalprojectCS65W

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PYq5kRWCJaNsVHhCSC1vqBaSVGOOoMcj

#**Data Analysis with Python [CS 65 W]**
###Eric Armstrong
---
##2019 StackOverflow Developer Survey Analysis 
---


#**Introduction**

> For the past few years, I've been working in the field of in-house technical recruiting. I help companies hire for software engineering positions that might be difficult to fill or that don't see the volume and quality of incoming applications they'd like. My day-to-day often consists of finding and engaging software developers through a variety of means, predominately online. With any luck, these software engineers are open to interviewing and potentially accepting an offer if interviews go well. 



> From full-stack, to data scientists & machine learning, infrastructure, mobile, and more, I've taken part in hiring for a range of positions within sofware engineering, and I've always been impressed with the diversity of backgrounds and technologies used in the various areas of the stack.




> In many ways, the recruiting industry has plenty of room to grow when it comes to matching developers to the right types of roles. Currently, companies rely on in-house and agency recruiters to find candidates and fill open roles, but in the future, automation may be relied upon more heavily for the discovery and assessment of future employees, especially as remote work opens up exponentially more options for developers and employers. 


> In order to get a more detailed and nuanced understanding of software engineers, I chose to do this analysis of the 2019 StackOverflow Developer Survey. The dataset can be downloaded and accessed here: https://insights.stackoverflow.com/survey

> "*With nearly 90,000 responses fielded from over 170 countries and dependent territories, our 2019 Annual Developer Survey examines all aspects of the developer experience from career satisfaction and job search to education and opinions on open source software.*"

##Let's Get Started


###Getting set up: 
> This involves setting up Google Authentication so we'll be able to access the survey data currently stored in Google Drive. We'll also import any libraries and frameworks needed.
"""

#GETTING SET UP: This block intalls and sets up Google Authentication, the first step to allow the file to be pulled from my drive. 
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

#GETTING SET UP: This block imports and mounts Google Drive through File Stream. 
#Go to the link, select the account you are using, and copy and paste the authorization code. 
from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
#GETTING SET UP: Here, we're importing the libraries and packages that will be needed later. 
# %matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import pandas_profiling
import pydotplus 
from scipy.cluster import hierarchy
import seaborn as sns
from sklearn import decomposition, preprocessing, cluster, tree
from yellowbrick.cluster.silhouette import SilhouetteVisualizer
import missingno as mn

"""##Creating the Data Frame: Part 1

> We'll store the survey as a Pandas dataframe, and in the block after that we'll select the survey columns we'd like to use in our analysis.
"""

#CREATING THE DATA FRAME: This block reads the csv file containing the survey dataset and stores it as a Pandas Dataframe. 
path = "/content/gdrive/My Drive/Data/survey_results_public.csv"
df = pd.read_csv(path)

#CREATING THE DATA FRAME: Here, we are selecting the columns we'd like to focus on with our first analysis. 
df = df[['Age','Country','OrgSize','YearsCode','YearsCodePro', 'EdLevel', 'CareerSat', 'JobSat', 'LastHireDate', 'ConvertedComp', 'WorkWeekHrs']]

#CREATING THE DATA FRAME: Let's take a quick look at the data and what responses look like. 
#88,888 responses, impressive!
df

"""##Cleaning the Data: Part 1

> See each block for a description of what is being dropped / filled / encoded and why.
"""

#CLEANING THE DATA: Clearly, not every respondent answered all the survey questions. Before we go further, there's a useful question we can ask: 
#What percentage of questions are missing?
df.isna().mean() * 100

#I want to see all the different levels of education. Later I can assign each of these different levels a numerical value. 
df.EdLevel.unique()

#CLEANING THE DATA: 

#Drop rows with any values missing ("Going Nuclear!")
#I realize there are several ways I could fill in or approximate missing values, but for now there are plenty of responses to work with. 
#After these drops, there will still be 12984 responses to analyze.
df = df.dropna()

#For now, I am only interested in looking at developers in the USA. This is the main focus of my work, and this is what will bring the most value for the type of company I work for. 
df =df[df.Country == 'United States']

#I want to remove rows were compensation is under $1000 per year. I'm specifically interested in career/professional developers for this analysis
#During my investigation, I also noticed that a highly disproportionate number of respondents were reporting earning exactly 1 Million or 2 Million per year. 
#I am dropping these responses because I do not know which are being honest and what few of them actually make these amounts. 
df = df[df.ConvertedComp >1000]
df = df[df.ConvertedComp !=1000000]
df = df[df.ConvertedComp !=2000000]

#I want to remove anyone who reports working more than 168 hours per week, as this is not physically possible.
df = df[df.WorkWeekHrs < 168]

#I want to remove anyone reporting an age of more than 80. That's a little bit above where age tops out. There are several jokers claiming to be 100 years old. 
df = df[df.Age < 80]

#I want to remove anyone reporting an age of less than 15, as it's extremely unlikely anyone this age is taking a developer survey.
df = df[df.Age > 15]

#CLEANING THE DATA

#Here we are creating a function that transforms the contents of each of the columns. 
#This involves assigning values to categorical or ordinal responses, changing data types, and renaming a column

def tweak_survey(df):
  
  Age = (df.Age
         .astype(int))

  OrgSize = (df.OrgSize
             .replace({'Just me - I am a freelancer, sole proprietor, etc.':1, 
                       '2-9 employees':2,
                       '10 to 19 employees':3, 
                       '20 to 99 employees':4, 
                       '100 to 499 employees':5,
                       '500 to 999 employees':6,
                       '1,000 to 4,999 employees':7, 
                       '5,000 to 9,999 employees':8, 
                       '10,000 or more employees':9})
             .astype(int))
  
  EdLevel = (df.EdLevel
               .replace({'I never completed any formal education':0,
                         'Primary/elementary school':6,
                         'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)':12,
                         'Associate degree':14,
                         'Some college/university study without earning a degree':14,
                         'Bachelor’s degree (BA, BS, B.Eng., etc.)':16,
                         'Professional degree (JD, MD, etc.)':17,
                         'Master’s degree (MA, MS, M.Eng., MBA, etc.)':18,
                         'Other doctoral degree (Ph.D, Ed.D., etc.)':21})
                .astype(int))
  
  YearsCode = (df.YearsCode
               .replace({'Less than 1 year':0, 
                         'More than 50 years': 50})
               .astype(int))
  
  YearsCodePro = (df.YearsCodePro
                  .replace({'Less than 1 year':0, 
                            'More than 50 years': 50})
                  .astype(int))
  
  CareerSat = (df.CareerSat
               .replace({'Very dissatisfied':-2, 
                         'Slightly dissatisfied':-1,
                         'Neither satisfied nor dissatisfied':0, 
                         'Slightly satisfied':1, 
                         'Very satisfied':2,})
               .astype(int))
  
  JobSat = (df.JobSat
            .replace({'Very dissatisfied':-2, 
                      'Slightly dissatisfied':-1,
                      'Neither satisfied nor dissatisfied':0, 
                      'Slightly satisfied':1, 
                      'Very satisfied':2})
            .astype(int))
  
  LastHireDate = (df.LastHireDate
                   .replace({'I\'ve never had a job':0, 
                             'NA - I am an independent contractor or self employed':0,
                             'Less than a year ago':1, 
                             '1-2 years ago':2, 
                             '3-4 years ago':3,
                             'More than 4 years ago':4})
                   .astype(int))
  
  ConvertedComp = (df.ConvertedComp
                    .astype(int))
  

  WorkWeekHrs = (df.WorkWeekHrs
                 .rename('WorkWeekHours')
                 .astype(int))
  
  return pd.concat([Age, OrgSize, EdLevel, YearsCode, YearsCodePro, CareerSat, JobSat, LastHireDate, ConvertedComp, WorkWeekHrs], axis=1)


df2 = tweak_survey(df)

df2

#I want to see what types of data I have in each column
df2.dtypes

"""#**Analysis: Part 1 & Part 1.1**
#**General Information, Selected Demographics, Plots, and Correlations, and PCA**

> In this first section, we'll be looking at general descriptive information about the respondents. How old they are, what size organization they work in, how many years they've been coding and coding professionally, job and career satisfaction, the date they last changed jobs, compensation, and the hours they work per week



> Please note that in this first section, we are only focusing on developers in the United States.

##The "Average" Developer & Profiling Report

Here's the picture we can paint of the "average" survey respondent in the USA.

33.3 years old

16 years of education (Bachelor's Degree)

In an organization on the larger end of 500 to 999 employees

14.4 years experience coding (started coding sophomore year of college)

9.3 years coding professionally (coding pro a year out of college)

A little more than 'Slightly satisfied' with their career

A little under 'Slightly satisfied' with their current job

Last changed jobs a little more than 1-2 years ago

Earning just under $165k per year (nice!)

Working nearly 42 hours per week (longer than I thought)
"""

#ANALYSIS: If we want to take a shortcut, we can use the Pandas profiling report to get a general overview of the data. 
#This may give some interesting ideas on what to look into for the analysis. 
#If not, the sections below detail each component
pandas_profiling.ProfileReport(df2)

"""##Analysis: Part 1 - Basic Stuff
Descriptive Statistics & Plots
> Here we break each question down, look at the descriptive statistics, and find a suitable plot to visualize the data.

###Age
"""

#ANALYSIS: Age
#What is the general age distribution of respondents? 
df2.Age.describe()

#ANALYSIS: Age - let's make a boxplot 
df2.Age.plot.box()

#ANALYSIS: Age -histogram
#Looks like a pretty nice distribution to me
plt.hist(df2['Age'], edgecolor = 'black',
         bins = int(59/1))

"""###Organization Size"""

#ANALYSIS: OrgSize
df2.OrgSize.describe()

#ANALYSIS: OgrSize - let's make a boxplot  
df2.OrgSize.plot.box()

#ANALYSIS: OrgSize -histogram
#Looks like a pretty nice distribution to me
plt.hist(df2['OrgSize'], edgecolor = 'black',
         bins = int(9/1))

"""###Level of Education / Years of Education
Predominately Bachelor's degree level of education, followed by Master's or Associate's/Some college
"""

#ANALYSIS: Level of Education
#What is the general education distribution of respondents? 
df2.EdLevel.describe()

#ANALYSIS: EdLevel - let's make a boxplot
#Some outliers claiming to have absolutely no education at all
df2.EdLevel.plot.box()

#ANALYSIS: EdLevel - value counts
df2.EdLevel.value_counts(dropna=False)

#ANALYSIS: EdLevel - let's make a barplot instead
#That's better!
df2.EdLevel.value_counts(dropna=False).plot(kind='bar')

"""###Years Spent Coding
Half have under 12 years with a long tail, and blips at five years intervals
"""

#ANALYSIS: Years Spent Coding
df2.YearsCode.describe()

#ANALYSIS: Years Spent Coding - boxplot
df2.YearsCode.plot.box()

#ANALYSIS: Years Spent Coding -histogram
plt.hist(df2['YearsCode'], edgecolor = 'black', bins = int(50))

"""###Years Spent Coding Professionally
Half have under 7 years with a long tail, and minor blips at five years intervals
"""

#ANALYSIS: Years of Professional Coding 
df2.YearsCodePro.describe()

#ANALYSIS: Years of Professional Coding - boxplot
df2.YearsCodePro.plot.box()

#ANALYSIS: Years of Professional Coding -histogram
plt.hist(df2['YearsCodePro'], edgecolor = 'black',
         bins = int(50))

"""###Career Satisfaction
Most respondents are very satsified or slightly satisfied with their career.
"""

#ANALYSIS: Careers Satsifaction
df2.CareerSat.describe()

#ANALYSIS: Careers Satsifaction - barplot
df2.CareerSat.value_counts(dropna=False).plot(kind='bar')

#ANALYSIS: Careers Satsifaction - boxplot
df2.CareerSat.plot.box()

"""###Job Satsifaction
Most respondents are very satsified or slightly satisfied with their job.
"""

#ANALYSIS: Job Satsifaction
df2.JobSat.describe()

#ANALYSIS: Job Satisfaction - barplot
df2.JobSat.value_counts(dropna=False).plot(kind='bar')

#ANALYSIS:  Job Satsifaction - boxplot
df2.JobSat.plot.box()

"""###Last Hire Date
Most have changed roles under a year, 1-2 years ago, or more than 4 years ago.

Very few self empoyed people or people who have never had a job.
"""

#ANALYSIS: Last Hire Date
df2.LastHireDate.describe()



#ANALYSIS: Last Hire Date - value counts
df2.LastHireDate.value_counts(dropna=False)

#ANALYSIS: Last Hire Date - boxplot
df2.LastHireDate.plot.box()

#ANALYSIS: Last Hire Date - histogram
plt.hist(df2['LastHireDate'], edgecolor = 'black')

"""###Compensation

Now for the part everyone's interested in! 

Half the values fall between 80,000 and 150,000 with many repondents claiming expectionally high incomes. 

Compensation can sometimes be a signal for how 'good' and engineer is at their job. Many of the plots in the next section look for other signals that may influence or be influenced by compensation.
"""

#ANALYSIS: Compensation
df2.ConvertedComp.describe()

#ANALYSIS: Compensation
#What are the most commonly reported incomes
#They're all landing on nice even numbers
df2.ConvertedComp.value_counts(dropna=False)

#ANALYSIS: Compensation - boxplot
df2.ConvertedComp.plot.box()

#ANALYSIS: Compensation - histogram
plt.hist(df2['ConvertedComp'], edgecolor = 'black',
         bins = int(500/1))

logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))
plt.subplot(212)
plt.hist(df2, bins=logbins)
plt.xscale('log')
plt.show()

#ANALYSIS: Work Week Hours
df2.WorkWeekHours.describe()

#ANALYSIS: Work Week Hours - boxplot
df2.WorkWeekHours.plot.box()

"""##Analysis: Part 1.1 - Fancy Stuff
Correlation Coefficients and More Plots
> Here, we can investigate a few relationships by comparing two or more columns through correlations and a variety of plots

###Correlation Matrix
This can give us a few ideas of what to focus on.
The blocks below include a few of these interesting correlations or lack thereof.
"""

#Doing a plain table first. 
df2.corr()

#Now for a correlation map with a bit more of a visual component
#Looks like there may some slight negative correlation between job satisfaction and last hire date
corr = df2.corr()
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
);

"""###Age VS Education : 0.116145

There's a weak correlation.
"""

#Combine frequency and scatter plot
sns.jointplot(x='Age', y='EdLevel', data=df2)

#This one seems to tell a better story of the trend.
ax = sns.violinplot(x="EdLevel", y="Age", data=df2)



"""###Age VS Last Hire Date : 0.282893
Seems older people have been in or stuck around their jobs longer.
"""

#ANALYSIS PART 1: Age vs Last Hire Date
#This shows a good general picture, but we can include means and error bars in the next graph to give more clarity
ax = sns.violinplot(x="LastHireDate", y="Age", data=df2)

"""###Age VS Compensation : 0.045894
No correlation, but it's an interesting graph.
"""

#ANALYSIS PART 1: Age vs Compensation
#Is there a clear relationship between age and compensation?
with sns.axes_style('dark', {'axes.facecolor':'pink'}):
    fig, ax = plt.subplots(figsize=(12, 8))
    sns.boxplot(x='Age', y='ConvertedComp', data=df2, ax=ax)
    ax.tick_params(axis='x', labelrotation=90)

"""###Org Size VS Education : 0.122191
Something of a relationship here.
"""

#ANALYSIS PART 1: Level of Education vs Compensation
#Let's plot it
ax = sns.violinplot(x="EdLevel", y="OrgSize", data=df2)

"""###Org Size and Last Hire Date : 0.129136
This isn't the correlation you're looking for.
"""

#This makes sense. People who have never had a job / work as consultants / are self-employed do not have a last hire date. 
#This may slight correlation may be a side effect of the way the question is set up. We should exclude the 0's next time.
ax = sns.violinplot(x="LastHireDate", y="OrgSize", data=df2)

"""###Years Spent Coding VS Years Coding Professionally : 0.891636
The correlation is strong with this one. This is an obvious finding.
"""

#ANALYSIS PART 1: Years Spent Coding VS. Years Spent Coding Professionally - lmplot with Seaborn 
#Here, we can see there are a few outliers who report spending more years coding professionally than they've spent coding in total...
#Apart from that, the relationship is very clear!
sns.lmplot(x='YearsCode', y='YearsCodePro', data=df2)

"""###Years Spent Coding VS Last Hire Date : 0.295909
Nearly the same when you swap out Years Spent Coding Professionally
"""

ax = sns.violinplot(x="LastHireDate", y="YearsCode", data=df2)

"""### Years Spent Coding Professionally VS Compensation : 0.065789
A very weak correlation.
"""

#ANALYSIS PART 1: Years Spent Coding Professionally VS. Compensation
sns.lmplot(x='YearsCodePro', y='ConvertedComp', data=df2)

"""### Years Spent Coding Professionally VS Work Week Hours : 0.103443"""

sns.lmplot(x='YearsCodePro', y='WorkWeekHours', data=df2)

"""###Career Satisfaction VS Job Satisfaction : 0.612174
There's a fairly strong correlation.
And apparently a ton of developers who are very satisfied with their jobs and careers. That's great!
"""

#ANALYSIS PART 1: Job Satisfaction VS Career Satisfaction
ax = sns.violinplot(x="JobSat", y="CareerSat", data=df2)

#ANALYSIS: Job Satisfaction VS Career Satisfaction
#Let's try another type of visualization for the same question
with sns.axes_style('dark', {'axes.facecolor':'pink'}):
    fig, ax = plt.subplots(figsize=(12, 8))
    sns.boxplot(x='JobSat', y='CareerSat', data=df2, ax=ax)
    ax.tick_params(axis='x')

"""###Job Satisfaction VS Last Hire Date : -0.097551
There a slight negative correlation. Could be a few things going on...

1) People who have been in their jobs too long are becoming dissatisfied with their work

2) People are more satisfied with their work when they're newer to the organization
"""

#There might be an explanation for this...
#I like the look of this one much better. It's modern art compared to the scatterplot I started with. 
#I know now when I have a low number of discreet values to compare, this is the way to go!
ax = sns.violinplot(x="JobSat", y="LastHireDate", data=df2)

"""###Compensation VS Work Week Hours : 0.068043
Is there a clear relationship between how long someone works and the amount they earn?
"""

df.corr()

#ANALYSIS PART 1: Work Week Hours VS Compensation
#I feel bad for the person working 160 hours per week and making less than the average compensation... 
sns.lmplot(x='WorkWeekHours', y='ConvertedComp', data=df2)



"""###Job Satisfaction VS Compensation : 0.006274
Absolutely no correlation, which I was not expecting to see!
"""

#We suspect that higher job satisfaction may positively correlated with compensation
#Turns out, there is not a very strong correlation!
#ANALYSIS: Job Satisfaction VS Compensation
#Density: not the best visualization in this case...
df2.plot.hexbin(x='JobSat', y='ConvertedComp', gridsize=20, cmap='viridis')

#ANALYSIS: Job Satisfaction VS Compensation
#I'm exporing using a logarithmic scale for this one...
g = sns.violinplot(x="JobSat", y="ConvertedComp",
                data=df2, palette="muted")
g.set_yscale("log")

"""###Years Professional Coding VS Compensation VS Job Satisfaction [TRIPLE]
Interesting to not see much of a trend or relationship here.
"""

sns.relplot(x='YearsCodePro', y='ConvertedComp', data=df2, hue='JobSat')

"""###Age VS Compensation VS Job Satisfaction [TRIPLE]"""

#ANALYSIS: Age VS Compensation VS Job Satisfaciton
#From these graphs, it appears that low job satisfaction may be linked to generally lower compensation outlooks
#The most satisfied respondents also had more outliers for higher compensation :)
sns.relplot(x='Age', y='ConvertedComp', data=df2, col='JobSat', col_wrap=2, alpha=.1)

"""###Principal Component Analysis
Let's take a look...
"""

#ANALYSIS PART 1 (Principal Component Analysis)
#PCA - passing in rows of features, new dataset will be the embeddings (linear combinations of columns that maximize the variance) 
#Reduces the dimensionality of the data
#Code below standardizes the data
X = df2
std = preprocessing.StandardScaler()
X_std = pd.DataFrame(std.fit_transform(X), columns=X.columns)
X_std

#ANALYSIS PART 1 (Principal Component Analysis)
#Make an instance of PCA
pca = decomposition.PCA()
pca_X = pd.DataFrame(pca.fit_transform(X_std), columns=[f'PC{i+1}' for i in range(len(X.columns))])
pca_X

#Nearly 3% of the variance can be explained
pca.explained_variance_ratio_

# What columns make up the components 1-6?
#PCA 1: Age, Years Coding, Years Coding Pro, Last Hire Date
#PCA 2: Lower career satisfaction, lower job satisfaction
#PCA 3: Smaller org size, lower education level, lower converted comp and work week hours
#PCA 4: Smaller Org Size, higher converted comp and work week hours
#PCA 5: Lower comp higher work week hours
#PCA 6: Lager org size lower educational level, high last hire date and comp
(pd.DataFrame(pca.components_, columns=X.columns)
 .iloc[0:6]
 .plot.bar()
 .legend(bbox_to_anchor=(1,1)))

#Looking into how our first two principal components stack up, and if age takes any trends
sns.scatterplot(x='PC1', y='PC2', 
                data=pca_X.assign(Age=X.Age), 
                hue='Age')

"""#**Analysis: Part 2 - The Tools of the Trade (PCA and Clustering for Technologies Developers Use!)**
---



> The StackOverflow Developer Survey also collected some very valuable data about the types of tools respondents use in their roles in software engineering, as well as their self-identified job titles. This includes coding languages, platforms, operating systems, web frameworks, databases, and miscelaneous technologies, and titles. 



> In this second part of the analysis, we will be looking briefly at how we can cluster these tools and titles to paint a clearer picture of the various types of software developers. Please note- this portion of the analysis will include developers world-wide, not only those in the USA. 



> We try three different methods for approximating how many clusters we can group our engineers into. Please note in advance that there was no difinitive or obvious best number to use for the number of clusters. Likely more are needed.



> We also do some principal component analysis, and find that up to nearly 7% of the variance can be explained by the first principal component for technologies used. 





---

##Creating the Data Frame: Part 2


> Again, we'll store the survey as a Pandas dataframe, and in the block after that we'll select a different set of columns we'd like to use in our analysis of tools & technologies.
"""

#ANALYSIS PART 2
#Storing the dataset as a Pandas Dataframe (once again, but with a different name
path = "/content/gdrive/My Drive/Data/survey_results_public.csv"
df = pd.read_csv(path)

#ANALYSIS PART 2
#Selecting only the columns we'd like to use for this analysis, storing as dataframe
#Removed, for now, PlatformWorkedWith, 'DatabaseWorkedWith', 'WebFrameWorkedWith','MiscTechWorkedWith', 'LanguageWorkedWith', 'Country'
df = df[['LanguageWorkedWith']]

# 'MiscTechWorkedWith', 'OpSys', 'DevType''PlatformWorkedWith' 'DatabaseWorkedWith','WebFrameWorkedWith'

"""##Cleaning the Data: Part 2

> See each block for a description of what is being dropped / filled / encoded and why.
"""

#ANALYSIS PART 2
#What percentage of values are missing? Checking for missing values
df.isna().mean() * 100

#ANALYSIS PART 2 (Data Cleaning)
#Dropping all empty values
df = df.dropna()

#ANALYSIS PART 2 (Data Encoding)
#Creating dummy variables for the high cardinality tech stack responses- eache technology now gets its own column
 
 def tweak_survey(df):
 
  #PlatformWorkedWith = df['PlatformWorkedWith'].str.get_dummies(sep=';')
  LanguageWorkedWith = df['LanguageWorkedWith'].str.get_dummies(sep=';')
  #WebFrameWorkedWith = df['WebFrameWorkedWith'].str.get_dummies(sep=';')
  #DatabaseWorkedWith = df['DatabaseWorkedWith'].str.get_dummies(sep=';')
  #MiscTechWorkedWith = df['MiscTechWorkedWith'].str.get_dummies(sep=';')
  #OpSys = df['OpSys'].str.get_dummies(sep=';')
  #DevType = df['DevType'].str.get_dummies(sep=';')

  return pd.concat([ LanguageWorkedWith], axis=1)

  #MiscTechWorkedWith, OpSys, DevType, PlatformWorkedWith


dfa = tweak_survey(df)

#ANALYSIS PART 2 (Data Cleaning)
#Removes the several columns 'other' columns for the tech stack 

dfb = tweak_survey(df).loc[:,~tweak_survey(df).columns.str.startswith('Other')]

#Can't have columns with the same name!
d = {'Windows': ['Windows1', 'Windows2']}
dfc = dfb.rename(columns=lambda c: d[c].pop(0) if c in d.keys() else c)

#Can't have columns with the same name!
d = {'MacOS': ['MacOS1', 'MacOS2']}
dfd = dfc.rename(columns=lambda c: d[c].pop(0) if c in d.keys() else c)

#ANALYSIS PART 2 (Data Cleaning)
#Checking for any missing values before doing principal component analysis
dfa.isna().any()

"""##Analysis: Part 2

###Principal Component Analysis
"""

#ANALYSIS PART 2 (Principal Component Analysis)
#PCA - passing in rows of features, new dataset will be the embeddings (linear combinations of columns that maximize the variance) 
#Reduces the dimensionality of the data
#Code below standardizes the data
X = dfa
std = preprocessing.StandardScaler()
X_std = pd.DataFrame(std.fit_transform(X), columns=X.columns)
X_std

#ANALYSIS PART 2 (Principal Component Analysis)
#Make an instance of PCA
pca = decomposition.PCA()
pca_X = pd.DataFrame(pca.fit_transform(X_std), columns=[f'PC{i+1}' for i in range(len(X.columns))])
pca_X

pca.explained_variance_ratio_

(pd.DataFrame(pca.components_, columns=X.columns)
 .iloc[0:6]
 .plot.bar()
 .legend(bbox_to_anchor=(1,1)))

"""###Determining the Appropriate Number of Clusters"""

#ANALYSIS PART 2 (Clustering)
#First method for estimating the appropriate number of clusters 
inerts = []
for i in range(2, 20):
    k = cluster.KMeans(n_clusters=i, random_state=42)
    k.fit(X_std)
    inerts.append(k.inertia_)
    
pd.Series(inerts).plot()

#ANALYSIS PART 2 (Clustering)
#Second method for estimating the appropriate number of clusters 
#This may take a while to run...
start, end = 21, 27
cols = 2
rows = ((end - start) // cols)
fix, axes = plt.subplots(rows, cols, figsize=(12,8))
axes = axes.reshape(cols * rows)
for i, k in enumerate(range(start, end), 0):
    ax = axes[i]
    sil = SilhouetteVisualizer(cluster.KMeans(n_clusters=k, random_state=42), ax=ax)
    sil.fit(X_std)
    sil.finalize()
plt.tight_layout()

#ANALYSIS PART 2 (Clustering- building a heirarchy)
#Try another mechanism
#Third method for estimating the appropriate number of clusters 
#This one also takes a while, you may need to increase RAM
fig, ax = plt.subplots(figsize=(10,8))
hierarchy.dendrogram(hierarchy.linkage(X_std, method='ward'),
                    truncate_mode='lastp', p=20, show_contracted=True)
pass  # here to hide return value of above

"""###Choosing and Implementing Our Desired Number of Clusters"""

#ANALYSIS PART 2 (K-Means Clustering)
# going to choose 12 clusters this time...
k13 = cluster.KMeans(n_clusters=12, random_state=42)
k13.fit(X_std)
labels = k13.predict(X_std)

#ANALYSIS PART 2 (K-Means Clustering)
#Assigning cluster labels to each row 
X.assign(label=labels)

#ANALYSIS PART 2 (K-Means Clustering)
#Looking at tool mean usage and variance for each cluster
(X.assign(label=labels)
  .groupby('label')
  .agg(['mean', 'var'])
  .T
)

#ANALYSIS PART 2 (K-Means Clustering)
#How many developers in each cluster?
pd.Series(labels).value_counts().sort_index()

#ANALYSIS PART 2 (K-Means Clustering)
# Add coloring to aid impact to clusters
(X.assign(label=labels)
  .groupby('label')
  .mean()
  .T
 .style.background_gradient(cmap='RdBu', axis=1)
)

"""##Developer Types

**Here are the clusters of developers we found, their counts, and their descriptions:**

1) DevOps / Cloud / SRE / System Admin: 3423

2) Full Stack Windows / C#.NET Developer: 11019

3) Full Stack Java Developer: 4643

4) Erlang/Elixer Developers & Business Executive: 493

5) Full Stack & Front End Web Developer: 4434

6) Data Scientists & Machine Learning: 2552

7) Back-end Data Engineer: 1478

8) Full Stack Ruby/Rails Developer: 2244

9) Enterprise, Desktop, Embedded/Hardware, or Game Developer: 3713

10) iOS Developer: 2024

11) Node & JavaScript Developer: 5490

12) Android Developer: 1000

#**Conclusion**



> Overall, this analysis was a great starting point for getting to know the developer community a bit better, or at least those who chose to respond to the 2019 StackOverflow Developer Survey, answering the questions we're intersted in investigating.


> If we can assume that most people are answering honestly, I was very surprised that compensation was generally higher than I had anticipated. Not only is the mean relatively high, but there are more high-income outliers than I thought. It was equally surprising that compensation was not closely tied to years of experience, age, or years spent coding professionally. It's possible there is some other factor at play- in the future, I may look at developer type to see if there is a strong assocaition there, but I'm not counting on it. Furthermore, it's possible that people were more likely to over-report than under-report their compensation, thus giving the average a bit of a lift. Or, that people with generally lower compensation more often than not simply chose not to report their compensation. 


> I also found something very interesting in the relationship between time spent in a role and job satisfaction. It took a few tries with visualizing the data to get the story to shine through, but I made a breakthrough with using violin plots. The more years since your last job move, the less satisfied you may be with your job. Many times in recruiting, I find myself making the assumption that someone who's been in a role for many years is satisfied and not looking for a change. This might not always be the case!




> In general, I was pretty satisfied with the cluster analysis of the different types of developers based on the tools they use. I did not suspect that C# developers were so cohesive in terms of the tools they use, nor so numerous. Other developer types seemed to show more diversity in the range of their tools that I had anticipated, but I was still able to identify the general types of developers base on their tools and probable functions. In the future, I think I may need to use more clusters. 



> For the future, I want to find a way to incorporate both aspects (Parts 1 and 2) in the same analysis, rather than setting up two different dataframes. One barrier to doign this now was that some calculations -especially with the clustering- simply took too long, timed out, or ran out of usable ram. I will also consider finding ways to format the data to make these calculations run easier and faster.
"""